{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from VAEmodel import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enumd_msa.shape:  (320066, 79)\n",
      "seq_weights.shape:  (320066,)\n",
      "Number of sequences in loaded alignment:  320066\n",
      "Length of loaded alignment (final number of columns/a.a. positions in alignment for each sequence):  79\n",
      "<class 'tuple'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:75] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 20228171200 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(enumd_msa\u001b[39m.\u001b[39mshape))\n\u001b[1;32m     26\u001b[0m \u001b[39m# 2-dimensional latent space\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m vae \u001b[39m=\u001b[39m VAE(\u001b[39mlen\u001b[39;49m(AA_TYPES), \u001b[39m2\u001b[39;49m, enumd_msa\u001b[39m.\u001b[39;49mshape, [\u001b[39m200\u001b[39;49m, \u001b[39m200\u001b[39;49m])\n",
      "File \u001b[0;32m~/Public/ProtEvoVAE/proteinVAE/src/models/VAEmodel.py:116\u001b[0m, in \u001b[0;36mVAE.__init__\u001b[0;34m(self, n_aa_type, dim_latent, dim_msa, layers_n_hiddens)\u001b[0m\n\u001b[1;32m    110\u001b[0m decoder \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([nn\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m    111\u001b[0m     nn\u001b[39m.\u001b[39mLinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim_latent, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers_n_hiddens[\u001b[39m0\u001b[39m]),\n\u001b[1;32m    112\u001b[0m     nn\u001b[39m.\u001b[39mTanh()\n\u001b[1;32m    113\u001b[0m )])\n\u001b[1;32m    114\u001b[0m decoder\u001b[39m.\u001b[39mextend(hidden_layers)\n\u001b[1;32m    115\u001b[0m decoder\u001b[39m.\u001b[39mappend(nn\u001b[39m.\u001b[39mSequential(\n\u001b[0;32m--> 116\u001b[0m     nn\u001b[39m.\u001b[39;49mLinear(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers_n_hiddens[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdim_msa)\n\u001b[1;32m    117\u001b[0m ))\n\u001b[1;32m    118\u001b[0m decoder \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\u001b[39m*\u001b[39mdecoder)\n",
      "File \u001b[0;32m~/miniconda3/envs/ProtEvoVAE/lib/python3.10/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features \u001b[39m=\u001b[39m in_features\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_features \u001b[39m=\u001b[39m out_features\n\u001b[0;32m---> 96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39;49mempty((out_features, in_features), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs))\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m bias:\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty(out_features, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:75] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 20228171200 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "# Load the processed MSA data\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"../../data\")\n",
    "REF_SEQ_ID = \"PF00041\"\n",
    "AA_TYPES = ['R', 'H', 'K',\n",
    "      'D', 'E',\n",
    "      'S', 'T', 'N', 'Q',\n",
    "      'C', 'G', 'P',\n",
    "      'A', 'V', 'I', 'L', 'M', 'F', 'Y', 'W']\n",
    "\n",
    "enumd_msa = np.load(DATA_DIR / \"processed\" / \"enumd_mtx_{}.npy\".format(REF_SEQ_ID))\n",
    "print(\"enumd_msa.shape: \", enumd_msa.shape)\n",
    "seq_weights = np.load(DATA_DIR / \"processed\" / \"seq_weights_{}.npy\".format(REF_SEQ_ID))\n",
    "print(\"seq_weights.shape: \", seq_weights.shape)\n",
    "\n",
    "msa = MSA_Dataset(enumd_msa, seq_weights, np.array(AA_TYPES), MSA_to_OneHot)\n",
    "\n",
    "# check length = num of seqs\n",
    "print(\"Number of sequences in loaded alignment: \", len(msa))\n",
    "print(\"Length of loaded alignment (final number of columns/a.a. positions in alignment for each sequence): \", enumd_msa.shape[1])\n",
    "\n",
    "# 2-dimensional latent space\n",
    "vae = VAE(len(AA_TYPES), 2, enumd_msa.shape, [200, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ProtEvoVAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
